import numpy as np

class Dataset(object):
    def __init__(self, data_map, deterministic=False, shuffle=True):
        self.data_map = data_map
        self.deterministic = deterministic
        self.enable_shuffle = shuffle
        self.n = len(next(iter(data_map.values())))
        print('Dataset shape is:', self.n)
        self._next_id = 0
        self.shuffle()

    def shuffle(self):
        if self.deterministic:
            return
        perm = np.arange(self.n)
        np.random.shuffle(perm)

        for key in self.data_map:
            self.data_map[key] = self.data_map[key][perm]

        self._next_id = 0

    def next_batch(self, batch_size):
        if self._next_id >= self.n and self.enable_shuffle:
            self.shuffle()

        cur_id = self._next_id
        cur_batch_size = min(batch_size, self.n - self._next_id)
        self._next_id += cur_batch_size

        data_map = dict()
        for key in self.data_map:
            data_map[key] = self.data_map[key][cur_id:cur_id+cur_batch_size]
        return data_map

    def iterate_once(self, batch_size):
        if self.enable_shuffle: self.shuffle()

        while self._next_id <= self.n - batch_size:
            yield self.next_batch(batch_size)
        self._next_id = 0

    def subset(self, num_elements, deterministic=True):
        data_map = dict()
        for key in self.data_map:
            data_map[key] = self.data_map[key][:num_elements]
        return Dataset(data_map, deterministic)


def iterbatches(arrays, *, num_batches=None, batch_size=None, shuffle=True, include_final_partial_batch=True):
    assert (num_batches is None) != (batch_size is None), 'Provide num_batches or batch_size, but not both'
    arrays = tuple(map(np.asarray, arrays))
    n = arrays[0].shape[0]
    assert all(a.shape[0] == n for a in arrays[1:])
    inds = np.arange(n)
    if shuffle: np.random.shuffle(inds)
    sections = np.arange(0, n, batch_size)[1:] if num_batches is None else num_batches
    for batch_inds in np.array_split(inds, sections):
        if include_final_partial_batch or len(batch_inds) == batch_size:
            yield tuple(a[batch_inds] for a in arrays)


# class Dataset(object):
#     def __init__(self, data_map, deterministic=False, shuffle=True):
#         self.data_map = data_map
#         self.deterministic = deterministic
#         self.enable_shuffle = shuffle
#         self.n = len(next(iter(data_map.values())))
#         print('Dataset shape is:', self.n)
#
#     def _build_batch_set(self, batch_size):
#         i=0
#         data_batchs = []
#         while i <= self.n - batch_size:
#             data_map = dict()
#             for key in self.data_map:
#                 perm = np.arange(batch_size)
#                 np.random.shuffle(perm)
#
#                 data_map[key] = np.array(self.data_map[key][i:batch_size+i])
#                 data_map[key] = data_map[key][perm]
#
#             data_batchs.append(data_map)
#
#             i = batch_size + i
#
#         return data_batchs
#
#     def iterate_once(self, batch_size):
#         data_batchs = self._build_batch_set(batch_size)
#         batch_index = np.arange(len(data_batchs))
#         np.random.shuffle(batch_index)
#
#         for index in batch_index:
#             yield data_batchs[index]


if __name__ == "__main__":
    mb_encoder_batch = np.arange(100).reshape(20,5,1).tolist()
    mb_encoder_length = np.arange(100).reshape(20,5,1).tolist()

    mb_encoder_batch += np.arange(200).reshape(20,10,1).tolist()
    mb_encoder_length += np.arange(200).reshape(20,10,1).tolist()

    data_set = Dataset(dict(encoder_input=mb_encoder_batch,
                            encoder_length=mb_encoder_length),
                       deterministic=True, shuffle=False)

    for data in data_set.iterate_once(20):
        print(data['encoder_input'].shape)
        print(data['encoder_input'][0])